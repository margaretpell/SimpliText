{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CX8USzH3hR8",
        "outputId": "04cd4a9e-df40-4189-84bb-9ed00926c72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy>=6.1 (from wordfreq)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (3.3.0)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (1.0.7)\n",
            "Collecting regex>=2023.10.3 (from wordfreq)\n",
            "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1->wordfreq) (0.2.12)\n",
            "Installing collected packages: regex, locate, ftfy, wordfreq\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2023.6.3\n",
            "    Uninstalling regex-2023.6.3:\n",
            "      Successfully uninstalled regex-2023.6.3\n",
            "Successfully installed ftfy-6.1.3 locate-1.1.1 regex-2023.10.3 wordfreq-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wordfreq\n",
        "from wordfreq import word_frequency\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_freq(sentence):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  words = [tok.lemma_ for tok in nlp(sentence) if tok.pos_ not in [\"PUNCT\", \"SPACE\"]]\n",
        "  freq_dict = {}\n",
        "  for word in words:\n",
        "    freq = word_frequency(word, 'en')\n",
        "    freq_dict[word] = freq\n",
        "  vocab = dict(sorted(freq_dict.items(), key=lambda item: item[1]))\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "c8q0eR6T3wM_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Words Frequency\n",
        "import pandas as pd\n",
        "def compare_sentence_freq(sentence1, sentence2):\n",
        "    freq_dict1 = check_freq(sentence1)\n",
        "    freq_dict2 = check_freq(sentence2)\n",
        "    low_freq_count1 = sum(1 for freq in freq_dict1.values() if freq < 1e-4)\n",
        "    low_freq_count2 = sum(1 for freq in freq_dict2.values() if freq < 1e-4)\n",
        "    return 0 if low_freq_count1 > low_freq_count2 else 1\n",
        "\n",
        "def compare_min_word_freq(sentence1, sentence2):\n",
        "    freq_dict1 = check_freq(sentence1)\n",
        "    freq_dict2 = check_freq(sentence2)\n",
        "    min_freq1 = min(freq_dict1.values()) if freq_dict1 else float('inf')\n",
        "    min_freq2 = min(freq_dict2.values()) if freq_dict2 else float('inf')\n",
        "    return 0 if min_freq1 < min_freq2 else 1\n",
        "\n",
        "df_expl = pd.read_csv('explanation.csv')\n",
        "df_baseline = pd.read_csv('baseline.csv')\n",
        "count_compare_sentence_freq = 0\n",
        "count_compare_min_word_freq = 0\n",
        "for (sentence1, sentence2) in zip(df_expl.iloc[:, 1], df_baseline.iloc[:, 3]):\n",
        "    count_compare_sentence_freq += compare_sentence_freq(sentence1, sentence2)\n",
        "    count_compare_min_word_freq += compare_min_word_freq(sentence1, sentence2)\n",
        "\n",
        "rate_compare_sentence_freq = count_compare_sentence_freq / len(df_expl)\n",
        "rate_compare_min_word_freq = count_compare_min_word_freq / len(df_expl)\n",
        "print(\"Sucess rate for compare_sentence_freq:\", rate_compare_sentence_freq)\n",
        "print(\"Sucess rate for compare_min_word_freq:\", rate_compare_min_word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-L-emcT30Fd",
        "outputId": "63d57aed-02f6-42ee-e3b8-123fd9339914"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucess rate for compare_sentence_freq: 0.9292929292929293\n",
            "Sucess rate for compare_min_word_freq: 0.8383838383838383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def average_sentence_length(text):\n",
        "    sentences = text.split('.')\n",
        "    word_counts = [len(sentence.split()) for sentence in sentences if sentence.strip()]\n",
        "    if len(word_counts) == 0:\n",
        "        return 0\n",
        "    return sum(word_counts) / len(word_counts)"
      ],
      "metadata": {
        "id": "Y6fy9YTn7vzL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentence Length\n",
        "def compare_average_sentence_length(sentence1, sentence2):\n",
        "    avg_len1 = average_sentence_length(sentence1)\n",
        "    avg_len2 = average_sentence_length(sentence2)\n",
        "    return 0 if avg_len1 > avg_len2 else 1\n",
        "\n",
        "df_expl = pd.read_csv('explanation.csv')\n",
        "df_baseline = pd.read_csv('baseline.csv')\n",
        "results = [compare_average_sentence_length(sentence1, sentence2)\n",
        "           for sentence1, sentence2 in zip(df_expl.iloc[:, 1], df_baseline.iloc[:, 3])]\n",
        "rate_of_1 = sum(results) / len(results)\n",
        "print(\"Sucess rate:\", rate_of_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYlBDpvU8jYq",
        "outputId": "c05313d8-6a6a-4b40-f73b-9a115dfbb4c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucess rate: 0.5858585858585859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Readability\n",
        "!pip install textstat\n",
        "import textstat\n",
        "import pandas as pd\n",
        "\n",
        "def compare_readability(sentence1, sentence2):\n",
        "    score1 = textstat.flesch_kincaid_grade(sentence1)\n",
        "    score2 = textstat.flesch_kincaid_grade(sentence2)\n",
        "    return 0 if score1 > score2 else 1\n",
        "\n",
        "df_expl = pd.read_csv('explanation.csv')\n",
        "df_baseline = pd.read_csv('baseline.csv')\n",
        "results = [compare_readability(sentence1, sentence2)\n",
        "           for sentence1, sentence2 in zip(df_expl.iloc[:, 1], df_baseline.iloc[:, 3])]\n",
        "rate_of_1 = sum(results) / len(results)\n",
        "print(\"Sucess rate:\", rate_of_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqlW0pZd9PHG",
        "outputId": "ad14002a-bd60-43f1-e908-48fe42d2b1d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.14.0)\n",
            "Sucess rate: 0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexicalrichness"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HiWprm1cWeL",
        "outputId": "257d76ad-a5d5-494f-8609-c6f086bfd06d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lexicalrichness\n",
            "  Downloading lexicalrichness-0.5.1.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (1.11.4)\n",
            "Requirement already satisfied: textblob>=0.15.3 in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (0.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (3.7.1)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.0.0->lexicalrichness) (1.23.5)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob>=0.15.3->lexicalrichness) (3.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lexicalrichness) (2023.3.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (4.66.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lexicalrichness) (1.16.0)\n",
            "Building wheels for collected packages: lexicalrichness\n",
            "  Building wheel for lexicalrichness (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lexicalrichness: filename=lexicalrichness-0.5.1-py3-none-any.whl size=15414 sha256=f68299915784cafb399df96e570d37ea7c7b16628477babc02a147724be5cb04\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ba/80/d4dabc1bf242a672ffc00226a2303a7471bb841c0872b2c212\n",
            "Successfully built lexicalrichness\n",
            "Installing collected packages: lexicalrichness\n",
            "Successfully installed lexicalrichness-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Lexical Diversity\n",
        "from lexicalrichness import LexicalRichness\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_lexical_diversity_measures(sentence):\n",
        "    lex = LexicalRichness(sentence)\n",
        "    hdd_draws = min(lex.words, 50)\n",
        "    return {\n",
        "        'ttr': lex.ttr,  # Type-Token Ratio\n",
        "    }\n",
        "\n",
        "def compare_lexical_diversity(sentence1, sentence2, measure):\n",
        "    diversity1 = calculate_lexical_diversity_measures(sentence1)\n",
        "    diversity2 = calculate_lexical_diversity_measures(sentence2)\n",
        "    return 0 if diversity1[measure] > diversity2[measure] else 1\n",
        "\n",
        "df_expl = pd.read_csv('explanation.csv')\n",
        "df_baseline = pd.read_csv('baseline.csv')\n",
        "measures = ['ttr']\n",
        "for measure in measures:\n",
        "    results = [compare_lexical_diversity(sentence1, sentence2, measure)\n",
        "               for sentence1, sentence2 in zip(df_expl.iloc[:, 1], df_baseline.iloc[:, 3])]\n",
        "    rate_of_1 = sum(results) / len(results)\n",
        "    print(f\"Sucess rate for {measure.upper()}: {rate_of_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFcJJuS0ENuo",
        "outputId": "8decd632-a3e7-49a4-b26a-1453cad6b00f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucess rate for TTR: 0.5555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Manually Measured Criterion:\n",
        "##1. Accuracy and Factual Correctness: Both are accurate and align with the fact.\n",
        "##2. Relevance and Completeness: All our explanations are better for English beginner. It also includes some facts and explain hard words which make sentences easier to understand. All include key points.\n",
        "##3. Scores: our main models success rate is 97%."
      ],
      "metadata": {
        "id": "oYznPN0yeUO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
